{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLX Model Fine-Tuning with LoRA\n",
    "\n",
    "This notebook will guide you through the steps of loading a pre-trained model, modifying it with LoRA layers, and training it on a specific dataset. This process is crucial for adapting large pre-trained models to new tasks with relatively small datasets and computational resources. We will load a pre-trained model, modify it with LoRA layers, and train it on a specific dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting Up JupyterLab for MLX Fine-Tuning\n",
    "\n",
    "## Installation\n",
    "Before you can start fine-tuning models with MLX, you need to set up your environment. We recommend using JupyterLab for this tutorial as it provides a robust, interactive development environment for Jupyter notebooks.\n",
    "\n",
    "### Install JupyterLab\n",
    "If you haven't already installed JupyterLab, you can do so using Conda, a popular package and environment management system. Run the following command in your terminal:\n",
    "\n",
    "```bash\n",
    "conda install jupyterlab\n",
    "```\n",
    "\n",
    "*This command will install JupyterLab and all required dependencies in your Conda environment.*\n",
    "\n",
    "## Launch JupyterLab\n",
    "Once the installation is complete, you can launch JupyterLab by running:\n",
    "\n",
    "```bash\n",
    "jupyter lab\n",
    "```\n",
    "\n",
    "\n",
    "*This command starts the JupyterLab server and opens JupyterLab in your default web browser. You can create a new notebook by clicking on the \"New\" button and selecting \"Python 3\" from the dropdown menu.*\n",
    "\n",
    "## Next Steps\n",
    "With JupyterLab running, you can now proceed to the tutorial sections in this notebook to start fine-tuning your MLX model with LoRA layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Necessary Libraries and Modules\n",
    "Before we start, we need to import all necessary libraries and modules that will be used throughout this notebook. This includes standard libraries for handling files and JSON data, as well as specific modules from the MLX library for model loading, modification, and training. \n",
    "\n",
    "*Before we begin the tutorial, it's important to ensure that all necessary Python libraries are installed. This includes libraries for machine learning, data manipulation, and model training. We will install these from a `requirements.txt` file that lists all the dependencies.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone mlx-examples repo\n",
    "!git clone https://github.com/ml-explore/mlx-examples\n",
    "!python3 -m pip install -r ./mlx-examples/lora/requirements.txt\n",
    "\n",
    "# Install the necessary libraries from the requirements.txt file\n",
    "!python3 -m pip install -r MLX_Fine-Tuning/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries and modules\n",
    "import random\n",
    "from typing import Tuple\n",
    "from mlx_lm import load  # Load function to load models\n",
    "from mlx_lm.tuner.lora import LoRALinear  # LoRA module for linear transformations\n",
    "from mlx.utils import tree_flatten  # Utility to flatten model parameters\n",
    "from mlx_lm.tuner.trainer import TrainingArgs, train  # Training utilities\n",
    "import mlx.optimizers as optim  # Optimizers for model training\n",
    "import json  # Module to work with JSON data\n",
    "from pathlib import Path  # Module for handling filesystem paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Class Definition\n",
    "Here we define a `Dataset` class to handle data operations. This class will be responsible for loading and accessing our dataset. It takes a list of data items and a key under which text data is stored. This abstraction allows us to easily fetch data by index and get its length, which are essential operations during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of the Dataset class to handle data operations\n",
    "class Dataset:\n",
    "    def __init__(self, data, key: str = \"text\"):\n",
    "        self._data = data\n",
    "        self._key = key\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        return self._data[idx][self._key]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Dataset\n",
    "To train our model, we first need to load our training and validation datasets. This function `load_dataset` takes a file path as input, checks for the file's existence, and reads the data. It returns an instance of the `Dataset` class filled with the loaded data. This setup is crucial for handling data efficiently during model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load a dataset from a specified path\n",
    "def load_dataset(path: str):\n",
    "    path = Path(path)\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"File not found: {path}\")\n",
    "    with open(path, \"r\") as fid:\n",
    "        data = [json.loads(line) for line in fid]\n",
    "    return Dataset(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up the Model and Data\n",
    "In this cell, we define the `setup` function which initializes and returns essential components for our training: the model, tokenizer, and datasets. We load a pre-trained model and tokenizer from a specified path and load both training and validation datasets using the previously defined `load_dataset` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function setup\n",
    "def setup():\n",
    "    train_dataset_path = \"./data/dorian_training_dataset.jsonl\"\n",
    "    val_dataset_path = \"./data/dorian_tvalid_dataset.jsonl\"\n",
    "    model_path = \"/Users/anima/DorainGray-Phi3-4k-MLX\"\n",
    "    model, tokenizer = load(model_path)\n",
    "    train_dst, valid_dst = load_dataset(train_dataset_path), load_dataset(val_dataset_path)\n",
    "    return model, tokenizer, train_dst, valid_dst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modifying the Model with LoRA\n",
    "In this section, we will modify the pre-trained model by integrating LoRA layers. LoRA allows us to adapt large pre-trained models with minimal additional parameters, making it efficient for fine-tuning on specific tasks. Below, we will freeze the original model parameters and add LoRA layers where necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the model with LoRA layers\n",
    "def modify_model_with_lora(model):\n",
    "    # Freeze the model to prevent updating weights of non-LoRA layers\n",
    "    model.freeze()\n",
    "    for l in model.model.layers:\n",
    "        # Iterate through each layer in the model\n",
    "        # Define the projections you want to update\n",
    "        projections = [\n",
    "            \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "            \"gate_proj\", \"up_proj\", \"down_proj\"\n",
    "        ]\n",
    "        \n",
    "        # Update self_attn projections if they exist\n",
    "        for proj in projections[:4]:  # For q_proj, k_proj, v_proj, o_proj\n",
    "            if hasattr(l.self_attn, proj):\n",
    "                # Replace existing linear layers with LoRALinear layers\n",
    "                setattr(l.self_attn, proj, LoRALinear.from_linear(\n",
    "                    getattr(l.self_attn, proj), r=64, alpha=128\n",
    "                ))\n",
    "        \n",
    "        # Update block_sparse_moe projections if they exist\n",
    "        if hasattr(l, \"block_sparse_moe\"):\n",
    "            for proj in projections[4:]:  # For gate_proj, up_proj, down_proj\n",
    "                if hasattr(l.block_sparse_moe, proj):\n",
    "                    # Replace existing linear layers with LoRALinear layers\n",
    "                    setattr(l.block_sparse_moe, proj, LoRALinear.from_linear(\n",
    "                        getattr(l.block_sparse_moe, proj), r=64, alpha=128\n",
    "                    ))\n",
    "            \n",
    "            # Update experts within block_sparse_moe\n",
    "            for e in l.block_sparse_moe.experts:\n",
    "                for proj in projections:  # Check all projections for each expert\n",
    "                    if hasattr(e, proj):\n",
    "                        # Replace existing linear layers with LoRALinear layers\n",
    "                        setattr(e, proj, LoRALinear.from_linear(\n",
    "                            getattr(e, proj), r=64, alpha=128\n",
    "                        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Configuration and Execution\n",
    "Now that our model has been modified to include LoRA layers, we need to set up the training configuration. This includes defining the training arguments, learning rate schedule, and optimizer. We will then proceed to train the model using the specified training and validation datasets. The training process is monitored by evaluating the model periodically and saving the model at specified intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure and execute training\n",
    "def train_model(model, tokenizer, train_dst, valid_dst):\n",
    "    trainingArgs = TrainingArgs(\n",
    "        batch_size=1,\n",
    "        iters=5000,\n",
    "        val_batches=25,\n",
    "        steps_per_report=10,\n",
    "        steps_per_eval=200,\n",
    "        steps_per_save=200,\n",
    "        adapter_file=\"adapters.npz\",\n",
    "        max_seq_length=4096,\n",
    "    )\n",
    "    decay_steps = trainingArgs.iters\n",
    "    lr_schedule = optim.cosine_decay(1e-5, decay_steps)\n",
    "    opt = optim.AdamW(learning_rate=lr_schedule)\n",
    "\n",
    "    \n",
    "    train(model=model, \n",
    "          tokenizer=tokenizer, \n",
    "          args=trainingArgs, \n",
    "          optimizer=opt, \n",
    "          train_dataset=train_dst, \n",
    "          val_dataset=valid_dst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Executing the Main Function\n",
    "Finally, we execute the main function which orchestrates the setup, model modification, and training process. This cell will trigger all the defined functions and start the model training process. Watch the outputs for progress and any potential issues that might need debugging.\n",
    "\n",
    "### The saved adapaters will appear in your directory as training progresses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute main function\n",
    "model, tokenizer, train_dst, valid_dst = setup()\n",
    "modify_model_with_lora(model)\n",
    "train_model(model, tokenizer, train_dst, valid_dst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fuse Trained Adapters to the Base Model\n",
    "\n",
    "After training adapters for specific tasks, you can fuse these adapters to the base model. This step integrates the specialized capabilities of the adapters directly into the model, which in turn creates a single model that can be used for inference. This model will be used as the starting point for conversion into GGUF format. This allows us to interact with it locally!\n",
    "\n",
    "### Breakdown\n",
    "\n",
    "- `python3`: This invokes the Python interpreter to run the script.\n",
    "\n",
    "- `./mlx-examples/lora/fuse.py`: This is the path to the Python script that handles the fusion of adapters to the base model.\n",
    "\n",
    "- `--model ./path/to/model`: Specifies the path to the base model file. This should be the path where the pre-trained or previously fine-tuned model is stored.\n",
    "\n",
    "- `--save-path ./new-fused-model-name`: This option sets the path and name for the output model file after the fusion process. This file will contain the base model with the adapters integrated.\n",
    "\n",
    "- `--de-quantize`: This flag indicates that if the model is quantized, it should be de-quantized before fusion. This is often necessary to ensure compatibility between the model and the adapters.\n",
    "\n",
    "- `--adapter-file ./adapters.npz.safetensors`: Specifies the path to the adapter file. This file contains the trained adapter parameters that will be fused with the base model.\n",
    "\n",
    "### Customization Options\n",
    "\n",
    "- **Model Path (`--model`)**: You can specify different models to which you want to apply the adapters, allowing for flexibility in experimenting with various base models.\n",
    "\n",
    "- **Output Path (`--save-path`)**: Adjust this path based on where you want to store the fused model. This is useful for organizing different versions or types of fused models.\n",
    "\n",
    "- **De-quantization (`--de-quantize`)**: This option can be toggled based on whether the input model is quantized. If your workflow involves models that are not quantized, this flag can be omitted.\n",
    "\n",
    "- **Adapter File (`--adapter-file`)**: This path can be changed to point to different adapter files, allowing you to fuse various adapters with the base model depending on the specific enhancements or customizations you've developed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 ./mlx-examples/lora/fuse.py --model ./path/to/model \\\n",
    "    --save-path ./new-fused-model-name \\\n",
    "    --de-quantize \\\n",
    "    --adapter-file ./adapters.npz.safetensors"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLX Model Fine-Tuning with LoRA\n",
    "\n",
    "This notebook will guide you through the steps of loading a pre-trained model, modifying it with LoRA layers, and training it on a specific dataset. This process is crucial for adapting large pre-trained models to new tasks with relatively small datasets and computational resources. We will load a pre-trained model, modify it with LoRA layers, and train it on a specific dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting Up Jupyter Notebook for MLX Fine-Tuning\n",
    "\n",
    "## Installation\n",
    "Before you can start fine-tuning models with MLX, you need to set up your environment. We recommend using JupyterLab for this tutorial as it provides a robust, interactive development environment for Jupyter notebooks.\n",
    "\n",
    "### Ensure Python is installed on your system.\n",
    "\n",
    "- Create the MLX environment:\n",
    "\n",
    "```bash\n",
    "conda create -n mlx-lora -y\n",
    "```\n",
    "\n",
    "- Activate the MLX environment:\n",
    "\n",
    "```bash\n",
    "conda activate mlx-lora\n",
    "```\n",
    "\n",
    "### Install Jupyter Notebook\n",
    "If you haven't already installed JupyterLab, you can do so using Conda, a popular package and environment management system. Run the following command in your terminal:\n",
    "\n",
    "```bash\n",
    "conda install jupyter notebook\n",
    "```\n",
    "\n",
    "*This command will install JupyterLab and all required dependencies in your Conda environment.*\n",
    "\n",
    "## Launch Jupyter Notebook\n",
    "Once the installation is complete, you can launch JupyterLab by running:\n",
    "\n",
    "```bash\n",
    "jupyter notebook\n",
    "```\n",
    "\n",
    "\n",
    "*This command starts the Jupyter Notbook server and opens Jupyter Notebook in your default web browser. You can create a new notebook by clicking on the \"New\" button and selecting \"Python 3\" from the dropdown menu.*\n",
    "\n",
    "## Next Steps\n",
    "With Jupyter Notebook running, you can now proceed to the tutorial sections in this notebook to start fine-tuning your MLX model with LoRA layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Necessary Libraries and Modules\n",
    "Before we start, we need to import all necessary libraries and modules that will be used throughout this notebook. This includes standard libraries for handling files and JSON data, as well as specific modules from the MLX library for model loading, modification, and training. \n",
    "\n",
    "*Before we begin the tutorial, it's important to ensure that all necessary Python libraries are installed. This includes libraries for machine learning, data manipulation, and model training. We will install these from a `requirements.txt` file that lists all the dependencies.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'mlx-examples'...\n",
      "remote: Enumerating objects: 3026, done.\u001b[K\n",
      "remote: Counting objects: 100% (1547/1547), done.\u001b[K\n",
      "remote: Compressing objects: 100% (406/406), done.\u001b[K\n",
      "remote: Total 3026 (delta 1349), reused 1196 (delta 1141), pack-reused 1479\u001b[K\n",
      "Receiving objects: 100% (3026/3026), 4.63 MiB | 2.40 MiB/s, done.\n",
      "Resolving deltas: 100% (2041/2041), done.\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/Users/anima/anaconda3/envs/txtai/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: mlx>=0.8.0 in /Users/anima/anaconda3/envs/txtai/lib/python3.10/site-packages (from -r ./mlx-examples/lora/requirements.txt (line 1)) (0.13.0)\n",
      "Requirement already satisfied: transformers in /Users/anima/anaconda3/envs/txtai/lib/python3.10/site-packages (from -r ./mlx-examples/lora/requirements.txt (line 2)) (4.40.0.dev0)\n",
      "Requirement already satisfied: numpy in /Users/anima/anaconda3/envs/txtai/lib/python3.10/site-packages (from -r ./mlx-examples/lora/requirements.txt (line 3)) (1.24.4)\n",
      "Requirement already satisfied: filelock in /Users/anima/anaconda3/envs/txtai/lib/python3.10/site-packages (from transformers->-r ./mlx-examples/lora/requirements.txt (line 2)) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /Users/anima/anaconda3/envs/txtai/lib/python3.10/site-packages (from transformers->-r ./mlx-examples/lora/requirements.txt (line 2)) (0.20.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/anima/anaconda3/envs/txtai/lib/python3.10/site-packages (from transformers->-r ./mlx-examples/lora/requirements.txt (line 2)) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/anima/anaconda3/envs/txtai/lib/python3.10/site-packages (from transformers->-r ./mlx-examples/lora/requirements.txt (line 2)) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/anima/anaconda3/envs/txtai/lib/python3.10/site-packages (from transformers->-r ./mlx-examples/lora/requirements.txt (line 2)) (2023.12.25)\n",
      "Requirement already satisfied: requests in /Users/anima/anaconda3/envs/txtai/lib/python3.10/site-packages (from transformers->-r ./mlx-examples/lora/requirements.txt (line 2)) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /Users/anima/anaconda3/envs/txtai/lib/python3.10/site-packages (from transformers->-r ./mlx-examples/lora/requirements.txt (line 2)) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Users/anima/anaconda3/envs/txtai/lib/python3.10/site-packages (from transformers->-r ./mlx-examples/lora/requirements.txt (line 2)) (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/anima/anaconda3/envs/txtai/lib/python3.10/site-packages (from transformers->-r ./mlx-examples/lora/requirements.txt (line 2)) (4.66.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/anima/anaconda3/envs/txtai/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers->-r ./mlx-examples/lora/requirements.txt (line 2)) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/anima/anaconda3/envs/txtai/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers->-r ./mlx-examples/lora/requirements.txt (line 2)) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/anima/anaconda3/envs/txtai/lib/python3.10/site-packages (from requests->transformers->-r ./mlx-examples/lora/requirements.txt (line 2)) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/anima/anaconda3/envs/txtai/lib/python3.10/site-packages (from requests->transformers->-r ./mlx-examples/lora/requirements.txt (line 2)) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/anima/anaconda3/envs/txtai/lib/python3.10/site-packages (from requests->transformers->-r ./mlx-examples/lora/requirements.txt (line 2)) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/anima/anaconda3/envs/txtai/lib/python3.10/site-packages (from requests->transformers->-r ./mlx-examples/lora/requirements.txt (line 2)) (2024.2.2)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/Users/anima/anaconda3/envs/txtai/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/Users/anima/anaconda3/envs/txtai/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: mlx in /Users/anima/anaconda3/envs/txtai/lib/python3.10/site-packages (0.13.0)\n",
      "Requirement already satisfied: mlx-lm in /Users/anima/anaconda3/envs/txtai/lib/python3.10/site-packages (0.0.13)\n",
      "Requirement already satisfied: torch in /Users/anima/anaconda3/envs/txtai/lib/python3.10/site-packages (2.1.2)\n",
      "Requirement already satisfied: numpy in /Users/anima/anaconda3/envs/txtai/lib/python3.10/site-packages (1.24.4)\n",
      "Requirement already satisfied: transformers in /Users/anima/anaconda3/envs/txtai/lib/python3.10/site-packages (4.40.0.dev0)\n",
      "Requirement already satisfied: protobuf in /Users/anima/anaconda3/envs/txtai/lib/python3.10/site-packages (from mlx-lm) (3.20.3)\n",
      "Requirement already satisfied: pyyaml in /Users/anima/anaconda3/envs/txtai/lib/python3.10/site-packages (from mlx-lm) (6.0.1)\n",
      "Requirement already satisfied: filelock in /Users/anima/anaconda3/envs/txtai/lib/python3.10/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /Users/anima/anaconda3/envs/txtai/lib/python3.10/site-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: sympy in /Users/anima/anaconda3/envs/txtai/lib/python3.10/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/anima/anaconda3/envs/txtai/lib/python3.10/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /Users/anima/anaconda3/envs/txtai/lib/python3.10/site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /Users/anima/anaconda3/envs/txtai/lib/python3.10/site-packages (from torch) (2023.10.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /Users/anima/anaconda3/envs/txtai/lib/python3.10/site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/anima/anaconda3/envs/txtai/lib/python3.10/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/anima/anaconda3/envs/txtai/lib/python3.10/site-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: requests in /Users/anima/anaconda3/envs/txtai/lib/python3.10/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /Users/anima/anaconda3/envs/txtai/lib/python3.10/site-packages (from transformers) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Users/anima/anaconda3/envs/txtai/lib/python3.10/site-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/anima/anaconda3/envs/txtai/lib/python3.10/site-packages (from transformers) (4.66.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/anima/anaconda3/envs/txtai/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/anima/anaconda3/envs/txtai/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/anima/anaconda3/envs/txtai/lib/python3.10/site-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/anima/anaconda3/envs/txtai/lib/python3.10/site-packages (from requests->transformers) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/anima/anaconda3/envs/txtai/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/anima/anaconda3/envs/txtai/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/Users/anima/anaconda3/envs/txtai/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Clone mlx-examples repo\n",
    "!git clone https://github.com/ml-explore/mlx-examples\n",
    "!python3 -m pip install -r ./mlx-examples/lora/requirements.txt\n",
    "\n",
    "# Install the necessary libraries from the requirements.txt file\n",
    "!pip install mlx mlx-lm mlx_lm torch numpy transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries and modules\n",
    "import random\n",
    "from typing import Tuple\n",
    "from mlx_lm import load  # Load function to load models\n",
    "from mlx_lm.tuner.lora import LoRALinear  # LoRA module for linear transformations\n",
    "from mlx.utils import tree_flatten  # Utility to flatten model parameters\n",
    "from mlx_lm.tuner.trainer import TrainingArgs, train  # Training utilities\n",
    "import mlx.optimizers as optim  # Optimizers for model training\n",
    "import json  # Module to work with JSON data\n",
    "from pathlib import Path  # Module for handling filesystem paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Class Definition\n",
    "Here we define a `Dataset` class to handle data operations. This class will be responsible for loading and accessing our dataset. It takes a list of data items and a key under which text data is stored. This abstraction allows us to easily fetch data by index and get its length, which are essential operations during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of the Dataset class to handle data operations\n",
    "class Dataset:\n",
    "    def __init__(self, data, key: str = \"text\"):\n",
    "        self._data = data\n",
    "        self._key = key\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        return self._data[idx][self._key]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Dataset\n",
    "To train our model, we first need to load our training and validation datasets. This function `load_dataset` takes a file path as input, checks for the file's existence, and reads the data. It returns an instance of the `Dataset` class filled with the loaded data. This setup is crucial for handling data efficiently during model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load a dataset from a specified path\n",
    "def load_dataset(path: str):\n",
    "    path = Path(path)\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"File not found: {path}\")\n",
    "    with open(path, \"r\") as fid:\n",
    "        data = [json.loads(line) for line in fid]\n",
    "    return Dataset(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up the Model and Data\n",
    "In this cell, we define the `setup` function which initializes and returns essential components for our training: the model, tokenizer, and datasets. We load a pre-trained model and tokenizer from a specified path and load both training and validation datasets using the previously defined `load_dataset` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function setup\n",
    "def setup():\n",
    "    train_dataset_path = \"./data/dorian_training_dataset.jsonl\"\n",
    "    val_dataset_path = \"./data/dorian_valid_dataset.jsonl\"\n",
    "    model_path = \"/Users/anima/DorainGray-Phi3-4k-MLX\"\n",
    "    model, tokenizer = load(model_path)\n",
    "    train_dst, valid_dst = load_dataset(train_dataset_path), load_dataset(val_dataset_path)\n",
    "    return model, tokenizer, train_dst, valid_dst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modifying the Model with LoRA\n",
    "In this section, we will modify the pre-trained model by integrating LoRA layers. LoRA allows us to adapt large pre-trained models with minimal additional parameters, making it efficient for fine-tuning on specific tasks. Below, we will freeze the original model parameters and add LoRA layers where necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlx_lm.tuner.lora import LoRALinear \n",
    "\n",
    "# Modify the model with LoRA layers\n",
    "def modify_model_with_lora(model):\n",
    "    # Freeze the model to prevent updating weights of non-LoRA layers\n",
    "    model.freeze()\n",
    "    for l in model.model.layers:\n",
    "        # Iterate through each layer in the model\n",
    "        # Define the projections you want to update\n",
    "        projections = [\n",
    "            \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "            \"gate_proj\", \"up_proj\", \"down_proj\"\n",
    "        ]\n",
    "        \n",
    "        # Update self_attn projections if they exist\n",
    "        for proj in projections[:]:  # For q_proj, k_proj, v_proj, o_proj\n",
    "            if hasattr(l.self_attn, proj):\n",
    "                # Replace existing linear layers with LoRALinear layers\n",
    "                setattr(l.self_attn, proj, LoRALinear.from_linear(\n",
    "                    getattr(l.self_attn, proj), r=32, lora_alpha=64\n",
    "                ))\n",
    "\n",
    "                        # Update additional custom projections\n",
    "        if hasattr(l, 'gate_proj'):\n",
    "            l.gate_proj = LoRALinear.from_linear(l.gate_proj, r=32, lora_alpha=64)\n",
    "        if hasattr(l, 'up_proj'):\n",
    "            l.up_proj = LoRALinear.from_linear(l.up_proj, r=32, lora_alpha=64)\n",
    "        if hasattr(l, 'down_proj'):\n",
    "            l.down_proj = LoRALinear.from_linear(l.down_proj, r=32, lora_alpha=64)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Configuration and Execution\n",
    "Now that our model has been modified to include LoRA layers, we need to set up the training configuration. This includes defining the training arguments, learning rate schedule, and optimizer. We will then proceed to train the model using the specified training and validation datasets. The training process is monitored by evaluating the model periodically and saving the model at specified intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure and execute training\n",
    "def train_model(model, tokenizer, train_dst, valid_dst):\n",
    "    trainingArgs = TrainingArgs(\n",
    "        batch_size=1,\n",
    "        iters=5000,\n",
    "        val_batches=25,\n",
    "        steps_per_report=10,\n",
    "        steps_per_eval=200,\n",
    "        steps_per_save=100,\n",
    "        adapter_file=\"adapters.npz\",\n",
    "        max_seq_length=4096,\n",
    "    )\n",
    "    decay_steps = trainingArgs.iters\n",
    "    lr_schedule = optim.cosine_decay(1e-5, decay_steps)\n",
    "    opt = optim.AdamW(learning_rate=lr_schedule)\n",
    "\n",
    "    \n",
    "    train(model=model, \n",
    "          tokenizer=tokenizer, \n",
    "          args=trainingArgs, \n",
    "          optimizer=opt, \n",
    "          train_dataset=train_dst, \n",
    "          val_dataset=valid_dst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Executing the Main Function\n",
    "Finally, we execute the main function which orchestrates the setup, model modification, and training process. This cell will trigger all the defined functions and start the model training process. Watch the outputs for progress and any potential issues that might need debugging.\n",
    "\n",
    "### The saved adapaters will appear in your directory as training progresses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training..., iters: 5000\n",
      "Iter 1: Val loss 1.258, Val took 47.805s\n",
      "Iter 10: Train loss 1.049, Learning Rate 1.000e-05, It/sec 0.046, Tokens/sec 95.092, Trained Tokens 20750\n",
      "Iter 20: Train loss 0.921, Learning Rate 1.000e-05, It/sec 0.054, Tokens/sec 109.715, Trained Tokens 41124\n",
      "Iter 30: Train loss 0.935, Learning Rate 9.999e-06, It/sec 0.053, Tokens/sec 115.291, Trained Tokens 62700\n",
      "Iter 40: Train loss 1.039, Learning Rate 9.998e-06, It/sec 0.045, Tokens/sec 107.939, Trained Tokens 86431\n",
      "Iter 50: Train loss 0.886, Learning Rate 9.998e-06, It/sec 0.054, Tokens/sec 117.412, Trained Tokens 108177\n",
      "Iter 60: Train loss 0.808, Learning Rate 9.997e-06, It/sec 0.068, Tokens/sec 129.936, Trained Tokens 127400\n",
      "Iter 70: Train loss 0.957, Learning Rate 9.995e-06, It/sec 0.048, Tokens/sec 111.106, Trained Tokens 150448\n",
      "Iter 80: Train loss 0.939, Learning Rate 9.994e-06, It/sec 0.048, Tokens/sec 110.488, Trained Tokens 173324\n",
      "Iter 90: Train loss 0.943, Learning Rate 9.992e-06, It/sec 0.045, Tokens/sec 111.149, Trained Tokens 198144\n",
      "Iter 100: Train loss 0.850, Learning Rate 9.990e-06, It/sec 0.071, Tokens/sec 135.419, Trained Tokens 217252\n",
      "Iter 110: Train loss 0.828, Learning Rate 9.988e-06, It/sec 0.065, Tokens/sec 124.696, Trained Tokens 236395\n",
      "Iter 120: Train loss 0.841, Learning Rate 9.986e-06, It/sec 0.073, Tokens/sec 139.154, Trained Tokens 255456\n",
      "Iter 130: Train loss 0.877, Learning Rate 9.984e-06, It/sec 0.066, Tokens/sec 132.895, Trained Tokens 275726\n",
      "Iter 140: Train loss 0.962, Learning Rate 9.981e-06, It/sec 0.045, Tokens/sec 112.205, Trained Tokens 300518\n",
      "Iter 150: Train loss 0.872, Learning Rate 9.978e-06, It/sec 0.068, Tokens/sec 137.664, Trained Tokens 320846\n",
      "Iter 160: Train loss 0.843, Learning Rate 9.975e-06, It/sec 0.068, Tokens/sec 139.086, Trained Tokens 341291\n",
      "Iter 170: Train loss 0.874, Learning Rate 9.972e-06, It/sec 0.057, Tokens/sec 123.124, Trained Tokens 363003\n",
      "Iter 180: Train loss 0.895, Learning Rate 9.968e-06, It/sec 0.057, Tokens/sec 135.529, Trained Tokens 386711\n",
      "Iter 190: Train loss 0.826, Learning Rate 9.965e-06, It/sec 0.076, Tokens/sec 152.926, Trained Tokens 406856\n",
      "Iter 200: Train loss 0.835, Learning Rate 9.961e-06, It/sec 0.068, Tokens/sec 137.415, Trained Tokens 427067\n",
      "Iter 200: Val loss 0.888, Val took 51.403s\n",
      "Iter 200: Saved adapter weights to checkpoints/200_adapters.npz.\n",
      "Iter 210: Train loss 0.831, Learning Rate 9.957e-06, It/sec 0.059, Tokens/sec 119.058, Trained Tokens 447148\n",
      "Iter 220: Train loss 0.938, Learning Rate 9.953e-06, It/sec 0.045, Tokens/sec 109.257, Trained Tokens 471254\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m model, tokenizer, train_dst, valid_dst \u001b[38;5;241m=\u001b[39m setup()\n\u001b[1;32m      3\u001b[0m modify_model_with_lora(model)\n\u001b[0;32m----> 4\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_dst\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[11], line 18\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, tokenizer, train_dst, valid_dst)\u001b[0m\n\u001b[1;32m     14\u001b[0m lr_schedule \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mcosine_decay(\u001b[38;5;241m1e-5\u001b[39m, decay_steps)\n\u001b[1;32m     15\u001b[0m opt \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdamW(learning_rate\u001b[38;5;241m=\u001b[39mlr_schedule)\n\u001b[0;32m---> 18\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m      \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrainingArgs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m      \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m      \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m      \u001b[49m\u001b[43mval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_dst\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/txtai/lib/python3.10/site-packages/mlx_lm/tuner/trainer.py:174\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, tokenizer, optimizer, train_dataset, val_dataset, args, loss, iterate_batches, training_callback)\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# Model update\u001b[39;00m\n\u001b[1;32m    172\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mupdate(model, grad)\n\u001b[0;32m--> 174\u001b[0m \u001b[43mmx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;66;03m# Record loss\u001b[39;00m\n\u001b[1;32m    177\u001b[0m losses\u001b[38;5;241m.\u001b[39mappend(lvalue\u001b[38;5;241m.\u001b[39mitem())\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Execute main function\n",
    "model, tokenizer, train_dst, valid_dst = setup()\n",
    "modify_model_with_lora(model)\n",
    "train_model(model, tokenizer, train_dst, valid_dst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fuse Trained Adapters to the Base Model\n",
    "\n",
    "After training adapters for specific tasks, you can fuse these adapters to the base model. This step integrates the specialized capabilities of the adapters directly into the model, which in turn creates a single model that can be used for inference. This model will be used as the starting point for conversion into GGUF format. This allows us to interact with it locally!\n",
    "\n",
    "### Breakdown\n",
    "\n",
    "- `python3`: This invokes the Python interpreter to run the script.\n",
    "\n",
    "- `./mlx-examples/lora/fuse.py`: This is the path to the Python script that handles the fusion of adapters to the base model.\n",
    "\n",
    "- `--model ./path/to/model`: Specifies the path to the base model file. This should be the path where the pre-trained or previously fine-tuned model is stored.\n",
    "\n",
    "- `--save-path ./new-fused-model-name`: This option sets the path and name for the output model file after the fusion process. This file will contain the base model with the adapters integrated.\n",
    "\n",
    "- `--de-quantize`: This flag indicates that if the model is quantized, it should be de-quantized before fusion. This is often necessary to ensure compatibility between the model and the adapters.\n",
    "\n",
    "- `--adapter-file ./adapters.npz.safetensors`: Specifies the path to the adapter file. This file contains the trained adapter parameters that will be fused with the base model.\n",
    "\n",
    "### Customization Options\n",
    "\n",
    "- **Model Path (`--model`)**: You can specify different models to which you want to apply the adapters, allowing for flexibility in experimenting with various base models.\n",
    "\n",
    "- **Output Path (`--save-path`)**: Adjust this path based on where you want to store the fused model. This is useful for organizing different versions or types of fused models.\n",
    "\n",
    "- **De-quantization (`--de-quantize`)**: This option can be toggled based on whether the input model is quantized. If your workflow involves models that are not quantized, this flag can be omitted.\n",
    "\n",
    "- **Adapter File (`--adapter-file`)**: This path can be changed to point to different adapter files, allowing you to fuse various adapters with the base model depending on the specific enhancements or customizations you've developed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m mlx_lm.fuse --model /Users/anima/DorainGray-Phi3-4k-MLX \\\n",
    "        --save-path ./DorainGray-Phi3-4k \\\n",
    "        --de-quantize \\\n",
    "        --adapter-file /Users/anima/Vodalus-Master-RAG-Wiki/MLX_Fine-Tuning/checkpoints/200_adapters.npz"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
